{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import urllib  \n",
    "import re  \n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "from sklearn.manifold import TSNE, MDS, Isomap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 0:  Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def extract(json):\n",
    "    \"\"\"\n",
    "    input: json object\n",
    "    output: (cve description, cve published data, cve ID)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (json['cve']['description']['description_data'][0]['value'], \n",
    "                pd.to_datetime(json['publishedDate']), json['cve']['CVE_data_meta']['ID'])\n",
    "    except:\n",
    "        return (json['description']['description_data'][0]['value'], \n",
    "                pd.to_datetime(json['publishedDate']), json['cve']['CVE_data_meta']['ID'])\n",
    "\n",
    "    \n",
    "#replace_dots: get rid of any extension '.'s so they are not interpreted as full-stops\n",
    "def replace_dots(text):\n",
    "    try:\n",
    "        ind = text.index('.')\n",
    "        while ind < len(text)-1:\n",
    "            if not text[ind+1:ind+2] == ' ' and not text[ind+1:ind+2] == '\"' and not text[ind+1:ind+2] == '\\'':\n",
    "                text = text[:ind] + '_' + text[ind+1:]\n",
    "            try:\n",
    "                ind = ind+1 + text[ind+1:].index('.')\n",
    "            except:\n",
    "                break\n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "    \n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'\\[?\\S+\\]?\\(?https?://\\S+\\)?', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_citations(text):\n",
    "    text = re.sub(r'\\(Citations?: \\S+\\)', '', text)\n",
    "    return text\n",
    "    \n",
    "    \n",
    "#clean up the text\n",
    "def remove_chars(text):\n",
    "    to_remove = \"This technique has been deprecated. Please see ATT&CK's Initial Access and Execution tactics for replacement techniques.\"\n",
    "    text = text.replace(to_remove,'')\n",
    "    text = re.sub('<[^>]*>', '', text.lower()).strip()\n",
    "    text = re.sub('[^a-zA-Z\\'\\_]', ' ', text.lower())\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    clean = remove_citations(text)\n",
    "    clean = remove_urls(clean)\n",
    "    clean = replace_dots(clean)\n",
    "    clean = remove_chars(clean)\n",
    "    return clean\n",
    "\n",
    "\n",
    "##****************MITRE Functions****************##\n",
    "\n",
    "def get_all_software(src):\n",
    "    filts = [\n",
    "        [Filter('type', '=', 'malware')],\n",
    "        [Filter('type', '=', 'tool')]\n",
    "    ]\n",
    "    return list(chain.from_iterable(\n",
    "        src.query(f) for f in filts\n",
    "    ))\n",
    "\n",
    "def get_all_techniques(src):\n",
    "    filt = [Filter('type', '=', 'attack-pattern')]\n",
    "    return src.query(filt)\n",
    "    \n",
    "def get_technique_by_name(src, name):\n",
    "    filt = [\n",
    "        Filter('type', '=', 'attack-pattern'),\n",
    "        Filter('name', '=', name)\n",
    "    ]\n",
    "    return src.query(filt)\n",
    "\n",
    "def get_techniques_by_content(src, content):\n",
    "    techniques = get_all_techniques(src)\n",
    "    return [\n",
    "        tech for tech in techniques\n",
    "        if content.lower() in tech.description.lower()\n",
    "    ]\n",
    "\n",
    "def get_mitigations_by_technique(src, tech_stix_id):\n",
    "    relations = src.relationships(tech_stix_id, 'mitigates', target_only=True)\n",
    "    return src.query([\n",
    "        Filter('type', '=', 'course-of-action'),\n",
    "        Filter('id', 'in', [r.source_ref for r in relations])\n",
    "    ])\n",
    "\n",
    "def get_group_by_technique(src, tech_stix_id):\n",
    "    relations = src.relationships(tech_stix_id, 'uses', target_only=True)\n",
    "    return src.query([\n",
    "        Filter('type', '=', 'intrusion-set'),\n",
    "        Filter('id', 'in', [r.source_ref for r in relations])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 1: Extract Data\n",
    "\n",
    "- Extract data from compressed files from NIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MITRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from stix2 import FileSystemSource\n",
    "from stix2 import Filter\n",
    "\n",
    "fs = FileSystemSource('enterprise-attack')\n",
    "filt = Filter('type', '=', 'attack-pattern')\n",
    "techniques_ent = fs.query([filt])\n",
    "\n",
    "fs = FileSystemSource('pre-attack')\n",
    "filt = Filter('type', '=', 'attack-pattern')\n",
    "techniques_pre = fs.query([filt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NIST NVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#read compressed files downloaded from NIST NVD \n",
    "\n",
    "mypath = 'nist_historical'\n",
    "onlyfiles = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "onlyfiles.remove('.DS_Store')\n",
    "\n",
    "\n",
    "blobs = {}\n",
    "\n",
    "for fname in onlyfiles:\n",
    "    with gzip.open(mypath+'/'+fname) as f:\n",
    "        blobs[fname] = json.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 2: Dedupe + Clean Data\n",
    "\n",
    "- Clean up ATT&CK data + add to dataframe\n",
    "- Dedupe CVE entries to account for any repetitions within the NIST compressed folders + add to dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MITRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ent_dict = {obj['id']: (obj['name'], obj['description']) for obj in techniques_ent}\n",
    "pre_dict = {obj['id']: (obj['name'], obj['description']) for obj in techniques_pre}\n",
    "\n",
    "ent_df = pd.DataFrame({'attack_id':ent_dict.keys(), 'values': ent_dict.values()})\n",
    "pre_df = pd.DataFrame({'attack_id':pre_dict.keys(), 'values': pre_dict.values()})\n",
    "\n",
    "ent_df['type'] = ['ent_attack']*ent_df.shape[0]\n",
    "pre_df['type'] = ['pre_attack']*pre_df.shape[0]\n",
    "\n",
    "techniques_df = pd.concat([ent_df, pre_df], axis=0)\n",
    "techniques_df['attack_name'] = techniques_df['values'].apply(lambda x: x[0].encode('utf-8').strip())\n",
    "techniques_df['attack_description'] = techniques_df['values'].apply(lambda x: x[1].encode('utf-8').strip())\n",
    "techniques_df.dropna(inplace=True)\n",
    "techniques_df['cleanText'] = techniques_df['attack_description'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## NIST NVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#cve_descs stores all of the cve descriptions from each JSON object\n",
    "cve_descs = {}\n",
    "\n",
    "for key in blobs.keys():\n",
    "    for n, report_meat in enumerate(blobs[key]['CVE_Items']):\n",
    "        desc, date, cve_id = extract(report_meat)\n",
    "        cve_descs[cve_id] = (desc,date)\n",
    "        \n",
    "#add all the values to a 3rd dataframe\n",
    "cve_df = pd.DataFrame({\"cveID\":cve_descs.keys(), \"cveDescription\": [v[0] for v in cve_descs.values()],\n",
    "                               \"publishedTime\": [v[1] for v in cve_descs.values()]})\n",
    "\n",
    "# final deduping of all 3 dataframes\n",
    "all_zip = zip(cve_df['cveID'], cve_df['cveDescription']) + zip(techniques_df['attack_id'],techniques_df['attack_description'])\n",
    "final_dict = {key: value for key,value in all_zip}\n",
    "\n",
    "# # add it to a datframe\n",
    "cve_df['cleanText'] = cve_df['cveDescription'].apply(lambda x: replace_dots(x))\n",
    "cve_df['cleanText'] = cve_df['cleanText'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 3: Create Named Tuples\n",
    "\n",
    "Gensim requires NamedTuples for processing documents. NamedTuples have the added convenience of associating tags with documents so it's easy to join documents back to their associated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "# this data object class suffices as a `TaggedDocument` (with `words` and `tags`) \n",
    "# plus adds other state helpful for our later evaluation/reporting\n",
    "Document = namedtuple('TaggedDocument', 'words tags object_type')\n",
    "\n",
    "alldocs = []\n",
    "for row_id, row in zip(techniques_df['attack_id'],techniques_df['cleanText']):\n",
    "    words = gensim.utils.to_unicode(row).split()\n",
    "    tags = [row_id]\n",
    "    object_type = 'ATTACK'\n",
    "    alldocs.append(Document(words, tags, object_type))\n",
    "    \n",
    "for row_id, row in zip(cve_df['cveID'],cve_df['cleanText']):\n",
    "    words = gensim.utils.to_unicode(row).split()\n",
    "    tags = [row_id] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "    object_type = 'CVE'\n",
    "    alldocs.append(Document(words, tags, object_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 4: Set up Doc2Vec Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We approximate the experiment of Le & Mikolov [\"Distributed Representations of Sentences and Documents\"](http://cs.stanford.edu/~quocle/paragraph_vector.pdf) with guidance from Mikolov's [example go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "`./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1`\n",
    "\n",
    "We vary the following parameter choices:\n",
    "* 100-dimensional vectors, as the 400-d vectors of the paper take a lot of memory and, in our tests of this task, don't seem to offer much benefit\n",
    "* Similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* Added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* A `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc10,t8) vocabulary scanned & state initialized\n",
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc10,t8) vocabulary scanned & state initialized\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc10,t8) vocabulary scanned & state initialized\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, vector_size=100, negative=5, hs=0, min_count=10, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, vector_size=100, window=10, negative=5, hs=0, min_count=10, sample=0, \n",
    "            epochs=20, workers=cores, alpha=0.05, comment='alpha=0.05'),\n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, vector_size=100, window=5, negative=5, hs=0, min_count=10, sample=0, \n",
    "            epochs=20, workers=cores),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "doc_list = alldocs[:]  \n",
    "shuffle(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec(dbow,d100,n5,mc10,t8)\n",
      "CPU times: user 2min 58s, sys: 38.5 s, total: 3min 37s\n",
      "Wall time: 1min 52s\n",
      "Training Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc10,t8)\n",
      "CPU times: user 4min 23s, sys: 55.3 s, total: 5min 18s\n",
      "Wall time: 2min 30s\n",
      "Training Doc2Vec(dm/c,d100,n5,w5,mc10,t8)\n",
      "CPU times: user 7min 58s, sys: 59.9 s, total: 8min 58s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "for model in simple_models: \n",
    "    print(\"Training %s\" % model)\n",
    "    %time model.train(doc_list, total_examples=len(doc_list), epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Step 5: Run with it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How similar are 'similar' words?\n",
    " Play around with words by changing the variable 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'execute' (25862 occurences)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zainabdanish/anaconda2/lib/python2.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dbow,d100,n5,mc10,t8)</th><th>Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc10,t8)</th><th>Doc2Vec(dm/c,d100,n5,w5,mc10,t8)</th></tr><tr><td>[(u'mdm', 0.4347737431526184),<br>\n",
       "(u'activematrix', 0.344107985496521),<br>\n",
       "(u'affected', 0.3438035845756531),<br>\n",
       "(u'uses', 0.31682759523391724),<br>\n",
       "(u'caused', 0.3130490779876709),<br>\n",
       "(u'awa', 0.31220555305480957),<br>\n",
       "(u'libvpx', 0.3115987181663513),<br>\n",
       "(u'gems', 0.31055089831352234),<br>\n",
       "(u'algorithms', 0.3047991991043091),<br>\n",
       "(u'_server', 0.3032001256942749),<br>\n",
       "(u'xserver', 0.30311456322669983),<br>\n",
       "(u'trigger', 0.30237576365470886),<br>\n",
       "(u'qualified', 0.29969215393066406),<br>\n",
       "(u'realplayer', 0.2984297275543213),<br>\n",
       "(u'co', 0.2973718047142029),<br>\n",
       "(u'share', 0.29577529430389404),<br>\n",
       "(u'grails', 0.29440414905548096),<br>\n",
       "(u'tries', 0.29240185022354126),<br>\n",
       "(u'h_', 0.2895732522010803),<br>\n",
       "(u'installers', 0.2866915762424469)]</td><td>[(u'run', 0.7800488471984863),<br>\n",
       "(u'inject', 0.7029496431350708),<br>\n",
       "(u'read', 0.6392181515693665),<br>\n",
       "(u'execution', 0.6188513040542603),<br>\n",
       "(u'overwrite', 0.5874972343444824),<br>\n",
       "(u'executing', 0.571281373500824),<br>\n",
       "(u'create', 0.49344170093536377),<br>\n",
       "(u'delete', 0.4572885036468506),<br>\n",
       "(u'signing', 0.4471280574798584),<br>\n",
       "(u'executed', 0.4363020062446594),<br>\n",
       "(u'perform', 0.4357762038707733),<br>\n",
       "(u'write', 0.4316786229610443),<br>\n",
       "(u'send', 0.41108831763267517),<br>\n",
       "(u'directories', 0.400007963180542),<br>\n",
       "(u'submit', 0.3983132839202881),<br>\n",
       "(u'embed', 0.394447922706604),<br>\n",
       "(u'deletion', 0.3891338109970093),<br>\n",
       "(u'change', 0.3856821060180664),<br>\n",
       "(u'load', 0.37896621227264404),<br>\n",
       "(u'download', 0.37731000781059265)]</td><td>[(u'run', 0.606421709060669),<br>\n",
       "(u'invoke', 0.5091031193733215),<br>\n",
       "(u'believe', 0.49609285593032837),<br>\n",
       "(u'executed', 0.4713059067726135),<br>\n",
       "(u'nnmrptconfig_exe', 0.46815118193626404),<br>\n",
       "(u'forge', 0.46805882453918457),<br>\n",
       "(u'plant', 0.4654300808906555),<br>\n",
       "(u'defeat', 0.4631540775299072),<br>\n",
       "(u'mgtconsole', 0.45738279819488525),<br>\n",
       "(u'executing', 0.4558832347393036),<br>\n",
       "(u'constrain', 0.45353806018829346),<br>\n",
       "(u'polkitsubject', 0.4528992474079132),<br>\n",
       "(u'hooked', 0.44935864210128784),<br>\n",
       "(u'wittenstein', 0.4491152763366699),<br>\n",
       "(u'purchasing', 0.4468623399734497),<br>\n",
       "(u'inject', 0.4468086361885071),<br>\n",
       "(u'programmatically', 0.44665005803108215),<br>\n",
       "(u'accomplish', 0.44530189037323),<br>\n",
       "(u'staticpages', 0.44160932302474976),<br>\n",
       "(u'recvfrom', 0.4409911036491394)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 200:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "word = 'execute'\n",
    "similars_per_model = [str(model.wv.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compare different models by printing closest documents\n",
    "The randomizer picks any document at random and prints the documents closest to it. The next two blocks use PV-DM and PV-DBOW respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PV-DM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (attack-pattern--8b57a8f1-9cbc-4b95-b162-cc2a1add94f2): «An adversary can run their code on systems with cyber security protections, such as antivirus products, in place to see if their code is detected. They can also test their malware on freely available public services. (Citation: MalwareQAZirtest)»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc10,t8):\n",
      "\n",
      "MOST (u'CVE-2017-3060', 0.6816185712814331): «Adobe Flash Player versions 25.0.0.127 and earlier have an exploitable memory corruption vulnerability in the ActionScript2 code parser. Successful exploitation could lead to arbitrary code execution.»\n",
      "\n",
      "MEDIAN (u'CVE-2018-3125', 0.4480755925178528): «Vulnerability in the Oracle Retail Merchandising System component of Oracle Retail Applications (subcomponent: Security (SQL Logger)). The supported version that is affected is 14.1. Easily exploitable vulnerability allows unauthenticated attacker with network access via HTTP to compromise Oracle Retail Merchandising System. Successful attacks of this vulnerability can result in unauthorized update, insert or delete access to some of Oracle Retail Merchandising System accessible data as well as unauthorized read access to a subset of Oracle Retail Merchandising System accessible data. CVSS 3.0 Base Score 6.5 (Confidentiality and Integrity impacts). CVSS Vector: (CVSS:3.0/AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:N).»\n",
      "\n",
      "LEAST (u'CVE-2012-4528', -0.03416857868432999): «The mod_security2 module before 2.7.0 for the Apache HTTP Server allows remote attackers to bypass rules, and deliver arbitrary POST data to a PHP application, via a multipart request in which an invalid part precedes the crafted data.»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "doc_id = np.random.randint(397)  # pick random doc, re-run cell for more examples\n",
    "model = simple_models[1]  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%s): «%s»\\n' % (alldocs[doc_id][1][0], final_dict[alldocs[doc_id][1][0]]))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], final_dict[sims[index][0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PV-DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (CVE-2017-5161): «An issue was discovered in Sielco Sistemi Winlog Lite SCADA Software, versions prior to Version 3.02.01, and Winlog Pro SCADA Software, versions prior to Version 3.02.01. An uncontrolled search path element (DLL Hijacking) vulnerability has been identified. Exploitation of this vulnerability could give an attacker access to the system with the same level of privilege as the application that utilizes the malicious DLL.»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dbow,d100,n5,mc10,t8):\n",
      "\n",
      "MOST (u'CVE-2018-14812', 0.8845694065093994): «An uncontrolled search path element (DLL Hijacking) vulnerability has been identified in Fuji Electric Energy Savings Estimator versions V.1.0.2.0 and prior. Exploitation of this vulnerability could give an attacker access to the system with the same level of privilege as the application that utilizes the malicious DLL.»\n",
      "\n",
      "MEDIAN (u'CVE-2013-3882', 0.2048366367816925): «Microsoft Internet Explorer 10 allows remote attackers to execute arbitrary code or cause a denial of service (memory corruption) via a crafted web site, aka \"Internet Explorer Memory Corruption Vulnerability,\" a different vulnerability than CVE-2013-3872, CVE-2013-3873, and CVE-2013-3885.»\n",
      "\n",
      "LEAST (u'CVE-2012-3298', -0.21165785193443298): «Unspecified vulnerability in the REST services framework in IBM WebSphere Commerce 7.0 Feature Pack 4 allows remote attackers to obtain sensitive information, modify data, or cause a denial of service via unspecified vectors.»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "  # pick random doc, re-run cell for more examples\n",
    "## 7189, 41355, 29820\n",
    "doc_id = 78030\n",
    "# doc_id = np.random.randint(simple_models[1].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = simple_models[0]  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%s): «%s»\\n' % (alldocs[doc_id][1][0], final_dict[alldocs[doc_id][1][0]]))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], final_dict[sims[index][0]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clusters\n",
    "- Create a 100 clusters\n",
    "- Explore each by changing the cluster index in the second cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "NUM_CLUSTERS = 100\n",
    "\n",
    "docvecs = []\n",
    "keys = []\n",
    "for doc_id in range(len(model.docvecs)):\n",
    "    keys.append(alldocs[doc_id][1][0])\n",
    "    docvecs.append(model.docvecs[doc_id])\n",
    "docvecs = np.array(docvecs)\n",
    "\n",
    "kclusterer = KMeans(n_clusters=NUM_CLUSTERS, random_state=333, max_iter=100)\n",
    "assigned_clusters = kclusterer.fit(docvecs)\n",
    "\n",
    "#add to dictionary\n",
    "key_cluster = {}\n",
    "for key,cluster in zip(keys, assigned_clusters.labels_):\n",
    "    if cluster in key_cluster:\n",
    "        key_cluster[cluster].append(key)\n",
    "    else: \n",
    "        key_cluster[cluster] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVE-2017-12414  >>  Format Factory 4.1.0 has a DLL Hijacking Vulnerability because an untrusted search path is used for msimg32.dll, WindowsCodecs.dll, and dwmapi.dll.\n",
      "\n",
      "CVE-2015-2369  >>  Untrusted search path vulnerability in Windows Media Device Manager in Microsoft Windows Server 2003 SP2, Windows Vista SP2, Windows Server 2008 SP2 and R2 SP1, and Windows 7 SP1 allows local users to gain privileges via a Trojan horse DLL in the current working directory, as demonstrated by a directory that contains a .rtf file, aka \"DLL Planting Remote Code Execution Vulnerability.\"\n",
      "\n",
      "CVE-2017-11160  >>  Multiple untrusted search path vulnerabilities in installer in Synology Assistant before 6.1-15163 on Windows allows local attackers to execute arbitrary code and conduct DLL hijacking attack via a Trojan horse (1) shfolder.dll, (2) ntmarta.dll, (3) secur32.dll or (4) dwmapi.dll file in the current working directory.\n",
      "\n",
      "CVE-2011-0452  >>  Untrusted search path vulnerability in the script function in Lunascape before 6.4.3 allows local users to gain privileges via a Trojan horse executable file in the current working directory.\n",
      "\n",
      "CVE-2011-0458  >>  Untrusted search path vulnerability in the Locate on Disk feature in Google Picasa before 3.8 allows local users to gain privileges via a Trojan horse executable file in the current working directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#change cluster number to see any random cluster\n",
    "cluster = 5\n",
    "for i in key_cluster[cluster][:5]:\n",
    "    print i, \" >> \", final_dict[i]\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Find Similar ATT&CK Pattern or CVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def find_similar(row_id, match_with, alldocs=alldocs, final_dict=final_dict, model=model):\n",
    "    \"\"\"\n",
    "    row_id: attack-pattern id or CVE id\n",
    "    match_with: 1=CVE, 2=attack\n",
    "    \"\"\"\n",
    "    ind = {1:'CVE', 2:'attack-pattern'}\n",
    "    doc_id = [i for i in range(len(alldocs)) if alldocs[i][1][0] == row_id]\n",
    "    sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)\n",
    "    sim_row_id = [i for i in sims if i[0][:3] == ind[match_with][:3]][0] #this ensures the top 1\n",
    "    print(\"**You asked for a(n) %s closest to the %s which is described as:**\" %(ind[match_with], row_id))\n",
    "    print\n",
    "    print(final_dict[row_id])\n",
    "    print\n",
    "    print(\"**The closest %s to %s is %s and is described as:**\"%(ind[match_with], row_id, sim_row_id[0]))\n",
    "    print\n",
    "    print(final_dict[sim_row_id[0]])\n",
    "    print\n",
    "    print(\"**Their similarity score is: %0.4f**\"%sim_row_id[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**You asked for a(n) CVE closest to the attack-pattern--aa8bfbc9-78dc-41a4-a03b-7453e0fdccda which is described as:**\n",
      "\n",
      "macOS and OS X use a common method to look for required dynamic libraries (dylib) to load into a program based on search paths. Adversaries can take advantage of ambiguous paths to plant dylibs to gain privilege escalation or persistence.\n",
      "\n",
      "A common method is to see what dylibs an application uses, then plant a malicious version with the same name higher up in the search path. This typically results in the dylib being in the same folder as the application itself. (Citation: Writing Bad Malware for OSX) (Citation: Malware Persistence on OS X)\n",
      "\n",
      "If the program is configured to run at a higher privilege level than the current user, then when the dylib is loaded into the application, the dylib will also run at that elevated level. This can be used by adversaries as a privilege escalation technique.\n",
      "\n",
      "**The closest CVE to attack-pattern--aa8bfbc9-78dc-41a4-a03b-7453e0fdccda is CVE-2017-6329 and is described as:**\n",
      "\n",
      "Symantec VIP Access for Desktop prior to 2.2.4 can be susceptible to a DLL Pre-Loading vulnerability. These types of issues occur when an application looks to call a DLL for execution and an attacker provides a malicious DLL to use instead. Depending on how the application is configured, the application will generally follow a specific search path to locate the DLL. The exploitation of the vulnerability manifests as a simple file write (or potentially an over-write) which results in a foreign executable running under the context of the application.\n",
      "\n",
      "**Their similarity score is: 0.7109**\n"
     ]
    }
   ],
   "source": [
    "find_similar('attack-pattern--aa8bfbc9-78dc-41a4-a03b-7453e0fdccda', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
